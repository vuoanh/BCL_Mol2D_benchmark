[bcl]
# specify bcl executable
bcl: /path/to/bcl/build/linux64_release/bin/bcl-apps-static.exe

[main]
# specify whether the bcl executable runs locally, uses pbs, or gnu-parallels for job execution
#local: 1
#host: antimony:
#gparallel: '-j4'
#pbs: '-W group_list=p_meiler'
#gparallel: '--slf ~/nodes_heavy.txt'
#max-pbs-jobs: 1400
#slurm
# If using the PBS scheduler, request 5 nodes at a time.  This is useful on clusters that prefer parallel jobs (e.g. piranha). 
# Set this number to at least 4 on piranha for best priority. The total number of jobs should ideally be divisible by this number
#bundle-jobs: 5

# set the complete flag to finish a cross-validation that was already started, but did not complete because some (or all) jobs failed
# If set, the script does not overwrite the output directories associated with this training ID, instead, only jobs that previously failed are run
# if all jobs completed, the script just exits after printing the objective function values
# E.g. if all but one model:Train call succeeded, just run the one failed instance.
# Unset this flag if you change the training configuration, because the internally-generated local/pbs scripts are not rewritten.
# The only change that will have an impact is increasing max-minutes or the override-memory-multiplier flag, because these change the parameters passed to the qsub command.
#complete:

#pbs:
#host: piranha
#opencl: Disable
random_seed: 
#pthreads: 3
#features: ../code_input_discrete_and_continuous_no_en.obj
#features: ./feature-scores/Cross_Validation_UtilityTest/rnd37_cols158.obj

[variables]
# activity cutoff used in objective functions
cutoff: 0.5
# parity determines whether values smaller then the cutoff are considered active (0) or inactive (1)
parity: 1
# choose one training objective function
#objective-function: 'RMSD'
#objective-function: 'Bootstrap(repeats=2000, function=AucRocCurve(cutoff=0.5,parity=1,x_axis_log=1,min fpr=0.001,max fpr=0.1), confidence interval=0.95)'
#objective-function: 'AucRocCurve(cutoff=0.5,parity=1,x_axis_log=1,min fpr=0.001,max fpr=0.1)'
objective-function: 'Bootstrap(repeats=2000, function=AucRocCurve(cutoff=0.5,parity=1,x_axis_log=1,min fpr=0.001,max fpr=0.1), confidence interval=0.95)'
#objective-function: 'BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=TPR),rhs=ContingencyMatrixMeasure(cutoff=%(cutoff)s,parity=%(parity)s,measure=TNR))'
#objective-function: 'EnrichmentAverage(cutoff=%(cutoff)s,enrichment max=0.01,step size=0.00001,parity=%(parity)s)'
#objective-function: 'InformationGainRatio(cutoff=%(cutoff)s,measure='PPV',parity=%(parity)s)'
visdrop: 0.05
hiddrop: 0.25
hiddenneurons: 32
alpha: 0.5
eta: 0.05
balanceratio: 0.1
droptype: Zero

[learning]
# choose one learning method
#learning-method: 'DecisionTree( objective function=%(objective-function)s,partitioner=Gini,activity cutoff=%(cutoff)s,node score = RatingTimesInitialNumIncorrect,min split = 5)'
#learning-method: 'SupportVectorMachine( objective function=%(objective-function)s, kernel = RBF( gamma=0.4),iterations=500,cost=0.1,gap_threshold=0.1)' 
#learning-method: 'KappaNearestNeighbor( objective function=%(objective-function)s, min kappa=1, max kappa=25)'
#learning-method: 'Kohonen( objective function=%(objective-function)s, map dimensions(20.0,20.0), steps per update=0, length=20, radius=10, neighbor kernel = Gaussian,initializer=RandomlyChosenElements)'
#learning-method: 'NeuralNetwork( transfer function = Sigmoid, weight update = Simple(eta=0.005,alpha=0.01), objective function = %(objective-function)s, steps per update=1, hidden architecture(8))'
#learning-method: 'NeuralNetwork(transfer function = Sigmoid, weight update = Simple(eta=0.1,alpha=0.5), objective function = %(objective-function)s, steps per update=0, hidden architecture(8))'
#learning-method: 'NeuralNetwork(transfer function = Sigmoid, weight update = Resilient, objective function = %(objective-function)s, steps per update=1, hidden architecture(8))'
#learning-method: 'OpenclIterateSequentialMinimalOptimization ( objective function=RMSD, gamma=0.4,iterations=500,cost=0.1 )'
#learning-method: 'OpenCLResilientPropagation(objective function=RMSD,steps per call=0, hidden architecture(8))'
#learning-method: 'OpenCLSimplePropagation(hidden architecture(8),eta=0.05,alpha=0.5,steps per call=0,objective function=RMSD)'

learning-method: 'NeuralNetwork( transfer function = Sigmoid, weight update = Simple(alpha=%(alpha)s,eta=%(eta)s),dropout(%(visdrop)s,%(hiddrop)s),objective function = %(objective-function)s,scaling=AveStd,steps per update=1,hidden architecture(%(hiddenneurons)s), balance=True,balance target ratio=%(balanceratio)s,shuffle=True,input dropout type=%(droptype)s)'

# for ANNs, the data selectors that work the best on qsar during descriptor selection are often one of :
# data selector=Balanced(pure classification=True,enrichment max=0.01,stability=1,all fn=True,all fp=True,max fp rounds=0,max fn rounds=0)
# OR 
# data selector=Accuracy(min fraction=1,noise above cutoff=0.025,noise below cutoff=0.025,pure classification = True) 
# better results may be had below by decreasing the max norm values.  The values here represent an upper limit to what normally works best for QSAR datasets. 
# A reasonable lower limit is MaxNorm(in=5,out=0.5) during training and MaxNorm(in=5,out=1) during pretraining
#learning-method: 'NeuralNetwork( transfer function = Sigmoid, weight update = Simple(eta=0.1,alpha=0.5),objective function = EnrichmentAverage(cutoff=4,enrichment max=0.015,step size=0.00001,parity=1), steps per update=1, hidden architecture(32),iteration weight update=MaxNorm(in=10,out=2), shuffle=True,data selector=Accuracy(min fraction=0.5,noise above cutoff=0.025,noise below cutoff=0.025,pure classification = True),initial network=NeuralNetwork( transfer function = Sigmoid, weight update = Simple(eta=0.1,alpha=0.5),iterations=110, steps per update=1, hidden architecture(32), shuffle=True,data selector=Accuracy(min fraction=0.5,noise above cutoff=0.025,noise below cutoff=0.025,pure classification = True),dropout(0.75,0.25),iteration weight update=MaxNorm(in=5,out=1)))'
#message_level: Verbose

# maximum training iterations of chosen learning-method
max-iterations: 100
monitor-independent-set:
#max_unimproved_iterations: 10
max-minutes: 40000
cv-repeats: 1
result_averaging_window: 0
#include-monitoring-in-training:
#swap-training-and-monitoring:
#features: ./feature-scores/Cross_Validation_UtilityTest/rnd26_cols405.obj

# choose one type of training data data assembly 
#balanced:
#combined:

# choose one final-objective-function that is applied on the finalized model
final-objective-function: '%(objective-function)s'
#final-objective-function: 'EnrichmentAverage(cutoff=%(cutoff)s,enrichment max=0.01,step size=0.0001,parity=%(parity)s)'
#final-objective-function: RMSD
#final-objective-function: 'InformationGainRatio(cutoff=%(cutoff)s,measure='PPV',parity=%(parity)s)'

[dataset]
# specify your dataset (.bin), listing just one or multiple datasets is allowed
datasets: ['your.bin']

#features: ../features.original

[score]
# choose one dataset scoring type 
scoring-type: InformationGain
#scoring-type: FScore
#scoring-type: InputSensitivity

# specify one output filename to store the dataset scoring information
output_score_file: score.infogain.out

# specify your features
#features: default_code_input.obj
#features: rnd18_cols294.obj
#features: ./feature-scores/IGR_TEST_PAM/rnd6_cols716.obj
#features: ./feature-scores/IGR_TEST_DiffEq_PAM/rnd16_cols247.obj

[descriptor-selection]
# specify a range [min,max,stepsize] to select the top n evaluated feature columns by score 
range: [10,100,10]

[cv]
monitoring-id-range: [0,4]
independent-id-range: [0,4]
#blind: [4,5]
cross-validations: 5
cv-repeats: 1
#training-size-limit: 1000000
#monitoring-size-limit: 1000000
override-memory-multiplier: 1.5
#message_level: Verbose

# where to store the model, options = Db, File, or None
store-model: File
id: ChangeMe
#remove-files-on-success:
print-independent-predictions:
show-status:
#features:  ./feature-scores/Cross_Validation_Utility/rnd29_cols314.obj
#features: /home/glickzl/bcl_vhts_qsar_workshop/qsar_cheminfo/1798_QSAR/feature-scores/Cross_Validation/rnd22_cols349.obj

[descriptor-selection-model-dependent]
# sign consistency among the ANN
# higher values give more weight to features that are used in consistent manners among the networks
weight-abs: 0
weight-utility: 0
weight-consistency: 0
weight-consistency-best: 1
#weight-sqr: 0 
attrition-rate: 0.0125

#set this flag to use input sensitivity
sensitivity: 
#is-feature-count: 600 
continue: 
#continue-after-scoring:

#threads to use when calculating input sensitivity
is-threads: 8 

#host to run scoring on
scoring-host: silicon

# minimum # of features to allow (default 1)
#min-features: MIN_FEATURES

# number of descriptor selection rounds to perform
ds-rounds: 150

# min and maximum features to remove per round (default 1 and 100, respectively)
#min-features-removed-per-round: MIN_FEATURES_REMOVED
max-features-removed-per-round: 800

# fraction of features to eliminate each round (bounded by --min-features-removed-per-round and --max-features-removed-per-round)
# defaults to max-features-removed-per-round / initial number of features
#attrition-rate: 0.05

# just as a note - the following listing shows all additional available flags for section [cv]

#training-size-limit TRAIN_SIZE_LIMIT
#monitoring-size-limit MON_SIZE_LIMIT
#monitoring-id-range <min> <max> 
#independent-id-range <min> <max>
#cross-validations <#CV> 
#balanced 
#combined
#features FEATURES
#score-file SCORE_FILE
#top-features TOP_N_FEATURES
#results RESULTS
#final-objective-function <objective-function>
#id <name>
#print-independent-predictions
#max-iterations ITERATIONS
#store-model {Db,File}
#store-metadata {Db,File}
#local [<# of jobs to run simultaneously, default= 12>
#pbs [<extra resource args to qsub>
#gparallel [<arguments to GNU parallels>
#host <hostname>
#max-reruns <RERUNS>
#opencl {Intel,ATI,NVIDIA,AMD,Disable}
#dry-run 
#round ROUND
#pthreads PTHREADS
#override-memory-multiplier: 1.2
memory-offset: 200
#remove-files-on-success
#show-status

#../bcl-all-static.exe model:Train 'NeuralNetwork( transfer function = Sigmoid, weight update = Resilient,dropout(0.0,0.5),objective function = BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure="TPR"),rhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure="TNR")),data selector=Accuracy(min fraction=1.0,tolerance above=0.4,pure classification = True), steps per update=0,hidden architecture(8), balance=True,balance target ratio=0.5,shuffle=True,initial network=NeuralNetwork( balance=True,balance target ratio=0.5,transfer function = Sigmoid, weight update = Resilient,iterations=110, steps per update=0, hidden architecture(8), shuffle=True,dropout(0.5,0.5)))' -max_minutes 30 -max_iterations 300 -feature_labels ../code_input_scalar_only.obj -opencl Disable --random_seed -final_objective_function 'BinaryOperation(op=*,lhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure=TPR),rhs=ContingencyMatrixMeasure(cutoff=3.5,parity=1,measure=TNR))' -training 'Chunks(number chunks=5,chunks="[0, 5) - [0] - [1]",dataset=Subset(number chunks=5,chunks="[0,5)-[4]",filename="/home/mendenjl/qsar_benchmark/data/1798.randomized.nointerpol.bin")) ' -monitoring 'Chunks(number chunks=5,chunks="[1]",dataset=Subset(number chunks=5,chunks="[0,5)-[4]",filename="/home/mendenjl/qsar_benchmark/data/1798.randomized.nointerpol.bin")) ' -independent 'Chunks(number chunks=5,chunks="[0]",dataset=Subset(number chunks=5,chunks="[0,5)-[4]",filename="/home/mendenjl/qsar_benchmark/data/1798.randomized.nointerpol.bin")) ' 
